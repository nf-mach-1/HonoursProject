# -*- coding: utf-8 -*-
"""HonoursProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aAlyK6enYa8hFxZjrXzVVlGhPxd7OKq
"""

#first open the google drive so that I can be able to access the dataset
from google.colab import drive
drive.mount('/content/drive')

"""**Importing all necessary packages and libraries**"""

#In this section we will import all the algorithms that will be evaluated in this model
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
import io


#Algorithms
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression, LogisticRegression


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_curve, f1_score, log_loss
from sklearn.metrics import roc_auc_score, auc
from sklearn.metrics import classification_report,confusion_matrix
from sklearn import tree
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import plot_confusion_matrix
from sklearn.decomposition import PCA

from pandas import Series
from sklearn.preprocessing import MinMaxScaler
from scipy import interp

"""**Reading the Dataset and Preprocessing**"""

datasets = pd.read_excel('/content/drive/MyDrive/data/dataset.xlsx')
datasets = datasets.drop(columns=['Ontology','Ontology Name'])
datasets.isna().sum()
datasets = datasets.dropna()

dataset = pd.DataFrame(datasets, columns=['noc','er','a/cr','irr','c/rr','anp','acr','ar', 'ir', 'rr', 'ap', 'cr', 'arc', 'alc', 'ad', 'md', 'ab', 'mb'])

def criterions(dataset):
  df = dataset.copy()
  km = KMeans(n_clusters=4,random_state=1, algorithm='full')
  #ypred = df._get_numeric_data()
  #km.fit(ypred)
  #y_pred = km.labels_
  y_pred = km.fit(df[['ar', 'ir', 'rr', 'ap', 'cr', 'arc', 'alc', 'ad', 'md', 'ab', 'mb']])
  df['criteria'] = y_pred.labels_
  print(y_pred.labels_)
  #print(y_pred)

  #two_dimension_pca = PCA(2)
  #transformed = two_dimension_pca.fit_transform(ypred)
  #plt.scatter(x=transformed[:,0],y=transformed[:,1],c=y_pred)
  return df


dataset = criterions(dataset)

"""**Algorithms to be used in this model**"""

names = ["k-Nearest Neighbor","Decision Tree","Naive Bayes","Support Vector Machine","Random Forest Classifier","Linear Regression","Logistic Regression"]

classifiers = [
    KNeighborsClassifier(5),
    DecisionTreeClassifier(max_depth=5),
    GaussianNB(),
    SVC(kernel="linear", C=0.025),
    SVC(kernel="poly", degree=3, C=0.025),
    SVC(kernel="rbf", C=1, gamma=2),
    RandomForestClassifier(max_depth=5, n_estimators=100),
    LinearRegression(),
    LogisticRegression(solver='liblinear',random_state=0)]

"""**Get the input and the output values for classification**

**Building the model, train and test it**

**Get the accuracy values for each model and display the classification report**
"""

xdata = dataset.drop(columns=['criteria'])
ydata = dataset['criteria']
xtrain,xtest,ytrain,ytest = train_test_split(xdata,ydata,test_size=0.3)
print(xtest.shape)

acc_score = []
con_score = []

for name, clf in zip(names, classifiers):
    clf.fit(xtrain, ytrain) 
    ascore = clf.predict(xtest) 
    acc_predicts = accuracy_score(ytest,ascore) 
    con_predicts = confusion_matrix(ytest,ascore)
    c_report = classification_report(ytest, ascore)
    print("The classification report for ",name)
    print(c_report)
    print()
    acc_score.append(acc_predicts)
    con_score.append(con_predicts)


df = pd.DataFrame()

df['Algorithm'] = names
df['Accuracy'] = acc_score

df = df.sort_values(by=['Accuracy'],ascending=False)
print(df) 
cm = sns.light_palette("red",as_cmap=True)
s = df.style.background_gradient(cmap = cm)

sns.set(style="whitegrid")
ax = sns.barplot(y="Algorithm",x="Accuracy",data=df)

"""**Implementation of Confusion Matrix for each algorithm**"""

def conf_Matrix(names, classifiers):
  yscore = 0
  for name, models in zip(names, classifiers):
    yscore = models.fit(xtrain, ytrain) 
    mdscore = clf.predict(xtest) 
    print("Confusion Matrix for ",name)
    plot_confusion_matrix(models, xtest, ytest)
    plt.show()
    print()

conf_Matrix(names, classifiers)

"""**Implement of the ROC Curve to compare the algorithms**


"""

def rf_nb_roc(xtrain, xtest, ytrain, ytest):
  rf = RandomForestClassifier(max_depth=5, n_estimators=100)
  rf.fit(xtrain, ytrain)
  nb = GaussianNB()
  nb.fit(xtrain, ytrain)
  knn = KNeighborsClassifier(5)
  knn.fit(xtrain, ytrain)

  r_probs = [0 for _ in range(len(ytest))]
  rf_probs = rf.predict_proba(xtest)
  nb_probs = nb.predict_proba(xtest)
  knn_probs = knn.predict_proba(xtest)

  rf_probs = rf_probs[:, 1]
  nb_probs = nb_probs[:, 1]
  knn_probs = knn_probs[:, 1]

  r_fpr, r_tpr, _ = roc_curve(ytest, r_probs, pos_label=True)
  r_auc = auc(r_fpr, r_tpr)
  rf_fpr, rf_tpr, _ = roc_curve(ytest, rf_probs, pos_label=True)
  rf_auc = auc(rf_fpr, rf_tpr)
  nb_fpr, nb_tpr, _ = roc_curve(ytest, nb_probs, pos_label=True)
  nb_auc = auc(nb_fpr, nb_tpr)
  knn_fpr, knn_tpr, _ = roc_curve(ytest, knn_probs, pos_label=True)
  knn_auc = auc(knn_fpr, knn_tpr)

  plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)
  plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest (AUROC = %0.3f)' % rf_auc)
  plt.plot(nb_fpr, nb_tpr, marker='.', label='Naive Bayes (AUROC = %0.3f)' % nb_auc)
  plt.plot(knn_fpr, knn_tpr, marker='.', label='K Nearest Neighbor (AUROC = %0.3f)' % knn_auc)

  plt.title('ALGORITHMS ROC CURVE PLOT')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.legend()
  plt.show()

rf_nb_roc(xtrain, xtest, ytrain, ytest)

def knn_roc_curve(xtrain, xtest, ytrain, ytest):
  return 0

knn_roc_curve(xtrain, xtest, ytrain, ytest)


xdata = dataset.drop(columns=['criteria'])
ydata = dataset['criteria']

y = label_binarize(ydata, classes=[0,1,2,3])

n_classes = 4
random_state = np.random.RandomState(0)
n_samples, n_features = xdata.shape
xdata = np.c_[xdata, random_state.randn(n_samples, 18*n_features)]
xtrain,xtest,ytrain,ytest = train_test_split(xdata, y, test_size=0.3, random_state=0)
def svm_roc_curve(xtrain, xtest, ytrain, ytest):
  model = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state))
  yscore = model.fit(xtrain, ytrain).decision_function(xtest)
  
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(ytest[:, i], yscore[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

  all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
  mean_tpr = np.zeros_like(all_fpr)
  for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])


  mean_tpr /= n_classes
  fpr["macro"] = all_fpr
  tpr["macro"] = mean_tpr
  roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

  plt.figure()
  lw = 2
  plt.plot(fpr["macro"], tpr["macro"], label='macro-average ROC Curve (area = {0:0.2f})' ''.format(roc_auc["macro"]), color='navy', linestyle=':',linewidth=4 )
  colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red'])
  for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC Curve of class {0} (area={1:0.2f})' ''.format(i, roc_auc[i]))


  plt.plot([0, 1], [0, 1], 'k--', lw=lw)
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title("Some Extension of Receiver operating characteristic to multi-class")
  plt.legend(loc="lower right")
  plt.show()


svm_roc_curve(xtrain, xtest, ytrain, ytest)
